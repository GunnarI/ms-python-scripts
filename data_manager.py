import os
import sys
import csv
import json
import warnings
import logging
from pathlib import Path
import numpy as np
import pandas as pd

root_dir = './'
cache_path = root_dir + 'cache/'


class DataManager:
    r"""
    The DataManager class contains references to the measured data and offers the possibility to read the relevant data
    from the experiments' .csv files, generated by the 'Export ASCII' operation in Vicon Nexus.

    A file called data_structure.json is created in project root directory when an instance is created for the very
    first time (or if the file does not exist already). This file holds a reference to the data structure of the
    Vicon Nexus experiment files and the name of each .csv file generated for each trial.

    The data structure itself should look something like the following example.

    Data Structure Example:
    ------
    | +- self.base_dir
    |   |   +- Subject01
    |   |   | +- 20190601
    |   |   |   | +- session_config.json
    |   |   |   | +- Subject01
    |   |   |   |   | +- Subject01 Walk01.csv
    |   |   |   |   | +- Subject01 Walk02.csv
    |   |   |   |   | +- ...
    |   |   | +- 20190705
    |   |   |   | +- session_config.json
    |   |   |   | +- Subject01
    |   |   |   |   | +- Subject01 Walk01.csv
    |   |   |   |   | +- ...
    |   |   | +- ...
    |   | +- Subject02
    |   | +- Subject03
    |   | +- ...

    Format example for session_config.json:
    ------
    | *session_config.json*
    {
        | "SessionID": "20190603",
        | "FramesPerSec": 100,
        | "AnalogFreq": 1000,
        | "EMGDevice": "A-Myon EMG - Voltage",
        | "NumEMG": 12,
        | "EMGProtocol":
        {
            | "EMG_A01": "GlutMax",
            | "EMG_A02": "VasMed",
            | "EMG_A03": "Semitend",
            | "EMG_A04": "BicFem",
            | "EMG_A05": "GasMed",
            | "EMG_A06": "GasLat",
            | "EMG_A07": "Soleus",
            | "EMG_A08": "TibAnt",
            | "EMG_B01": "RectFem",
            | "EMG_B02": "VasLat",
            | "EMG_B03": "PerLong",
            | "EMG_B04": "PerBrev"
        }
    }

    :param str base_dir: path to the directory where all subject folders from all experiments should be stored.
    E.g. self.base_dir='C:\\Users\\Gunnar\\Data'
    """
    def __init__(self, base_dir):
        self.base_dir = base_dir
        self.emg_data_dict = {}
        self.torque_data_dict = {}
        self.gait_cycles_dict = {}
        self.filt_data_dict = {}
        self.filt_concat_data_dict = {}
        self.list_of_pandas = {}

        if not os.path.exists(cache_path):
            os.makedirs(cache_path + '/dataframes')
            os.makedirs(cache_path + '/raw_data/input')
            os.makedirs(cache_path + '/raw_data/labels')

        if os.path.exists(root_dir + 'data_structure.json'):
            with open(root_dir + 'data_structure.json', 'r') as ds:
                self.data_struct = json.load(ds)
        else:
            self.data_struct = []
            self.update_data_structure()

    def update_data_structure(self):
        """
        Looks into the root directory of the experiment folders and updates the data structure with any changes that
        have been made.
        This includes updating self.data_struct dictionary as well as data_structure.json in project root
        """
        subject_folders = Path(self.base_dir).glob('Subject*')
        subject_ids = get_list_of_subject_ids(self.data_struct)

        for folder in subject_folders:
            subject_id = folder.stem

            if subject_id not in subject_ids:
                self.data_struct.append(create_subject(self.base_dir, subject_id))
            else:
                i = subject_ids.index(subject_id)
                self.data_struct[i] = update_subject(self.base_dir + '\\' + subject_id, self.data_struct[i])

        with open(root_dir + 'data_structure.json', 'w') as ds:
            json.dump(self.data_struct, ds, indent=4)

    def load_emg_and_torque(self, subject_id, session_id, reload=False):
        """
        Loads the raw emg data either from the original csv file or previously saved txt file.
        If txt file does not already exist then a new one is saved. All emg data (both from csv and txt files) is added
        to the DataManager.emg_data_dict dictionary, each trial with the key "<SessionID> <SubjectID> <TrialID>".
        Same goes for the torque data and DataManager.torque_data_dict.
        :param subject_id: the subject id from the data structure (e.g. "Subject01")
        :param session_id: the session id from the data structure (e.g. "20190405")
        :param reload: if True then the already saved txt files are overridden with values from the csv
        :return: nothing, only updates the class variables DataManager.emg_data_dict and DataManager.torque_data_dict
        """
        for subject in self.data_struct:
            if subject["SubjectID"] == subject_id:
                for session in subject["Sessions"]:
                    if session["SessionID"] == session_id:
                        emg_device = session["EMGDevice"]
                        num_emg = session["NumEMG"]
                        frame_freq = session["FramesPerSec"]
                        analog_freq = session["AnalogFreq"]
                        emg_headers = ['Time']
                        for key in session["EMGProtocol"]:
                            emg_headers.append(session["EMGProtocol"][key])
                        torque_headers = ['Time', 'MomentX', 'MomentY', 'MomentZ']

                        for trial in session["Trials"]:
                            session_trial_id = session_id + ' ' + subject_id + ' ' + trial["TrialID"]
                            emg_array_dir = cache_path + 'raw_data/input/' + session_trial_id + '.txt'

                            if 'walk' in trial["TrialID"].lower():
                                try:
                                    t1, t2, gait_cycles = get_gait_cycles(trial["File"])
                                except AssertionError as error:
                                    warnings.warn(str(error) + "\n\tThe trial was ignored!")
                                    continue

                                # Set gait_cycles_dict
                                self.gait_cycles_dict[session_trial_id] = {"gait_cycles": gait_cycles,
                                                                           "t1": t1, "t2": t2}

                                # Load torque data
                                torque_array_dir = cache_path + 'raw_data/labels/' + session_trial_id + '.txt'
                                if os.path.isfile(torque_array_dir) and not reload:
                                    self.torque_data_dict[session_trial_id] = {"headers": torque_headers,
                                                                               "data": np.loadtxt(torque_array_dir,
                                                                                                  skiprows=1),
                                                                               "t1": t1, "t2": t2,
                                                                               "gait_cycles": gait_cycles}
                                else:
                                    try:
                                        torque_data, torque_headers = get_torque_from_csv(trial["File"],
                                                                                          subject_id + ':RKneeMoment',
                                                                                          frame_freq)
                                    except AssertionError as error:
                                        warnings.warn(str(error) + "\n\tThe trial was ignored!")
                                        continue
                                    self.torque_data_dict[session_trial_id] = {"headers": torque_headers,
                                                                               "data": torque_data, "t1": t1,
                                                                               "t2": t2,
                                                                               "gait_cycles": gait_cycles}
                                    save_raw_data_to_txt(torque_data, torque_array_dir, headers=torque_headers)

                                # Load emg data
                                if os.path.isfile(emg_array_dir) and not reload:
                                    self.emg_data_dict[session_trial_id] = {"headers": emg_headers,
                                                                            "data": np.loadtxt(emg_array_dir,
                                                                                               skiprows=1),
                                                                            "t1": t1, "t2": t2,
                                                                            "gait_cycles": gait_cycles}
                                else:
                                    emg_data = get_emg_from_csv(trial["File"], emg_device, num_emg, frame_freq,
                                                                analog_freq)
                                    self.emg_data_dict[session_trial_id] = {"headers": emg_headers,
                                                                            "data": emg_data, "t1": t1, "t2": t2,
                                                                            "gait_cycles": gait_cycles}
                                    save_raw_data_to_txt(emg_data, emg_array_dir, headers=emg_headers)
                            else:
                                if os.path.isfile(emg_array_dir) and not reload:
                                    self.emg_data_dict[session_trial_id] = {"headers": emg_headers,
                                                                            "data": np.loadtxt(emg_array_dir,
                                                                                               skiprows=1)}
                                else:
                                    emg_data = get_emg_from_csv(trial["File"],
                                                                emg_device, num_emg, frame_freq, analog_freq)
                                    self.emg_data_dict[session_trial_id] = {"headers": emg_headers, "data": emg_data}
                                    save_raw_data_to_txt(emg_data, emg_array_dir, headers=emg_headers)
        self.write_raw_data_to_pandas()

    def write_raw_data_to_pandas(self):
        temp_dict = {'emg_raw_data': {}, 'torque_raw_data': {}, 'trial_cycle_names': {}}
        for key in self.torque_data_dict:
            ids = key.split()
            session_name = ids[0] + ' ' + ids[1]
            if session_name in temp_dict['trial_cycle_names'].keys():
                trial_cycle_names = temp_dict['trial_cycle_names'][session_name]
                emg_to_cache = temp_dict['emg_raw_data'][session_name]
                torque_to_cache = temp_dict['torque_raw_data'][session_name]
            else:
                trial_cycle_names = []
                emg_to_cache = [self.emg_data_dict[key]['headers']]
                torque_to_cache = [self.torque_data_dict[key]['headers']]

            trial_cycle_names.append(key)
            emg_to_cache.append(self.emg_data_dict[key]['data'])
            torque_to_cache.append(self.torque_data_dict[key]['data'])

            temp_dict['trial_cycle_names'][session_name] = trial_cycle_names
            temp_dict['emg_raw_data'][session_name] = emg_to_cache
            temp_dict['torque_raw_data'][session_name] = torque_to_cache

        for key in temp_dict['trial_cycle_names']:
            emg_trial_cycle_names = []
            torque_trial_cycle_names = []
            emg_columns = temp_dict['emg_raw_data'][key].pop(0)
            torque_columns = temp_dict['torque_raw_data'][key].pop(0)
            for i, trial_cycle_name in enumerate(temp_dict['trial_cycle_names'][key]):
                emg_trial_cycle_names.append([trial_cycle_name]*len(temp_dict['emg_raw_data'][key][i]))
                torque_trial_cycle_names.append([trial_cycle_name] * len(temp_dict['torque_raw_data'][key][i]))

            emg_data = np.concatenate(temp_dict['emg_raw_data'][key])
            emg_df = pd.DataFrame(data=emg_data, columns=emg_columns, dtype=float)
            emg_df = emg_df.assign(Trial=np.concatenate(emg_trial_cycle_names))
            emg_df.name = key + ' emg_raw_data'

            torque_data = np.concatenate(temp_dict['torque_raw_data'][key])
            torque_df = pd.DataFrame(data=torque_data, columns=torque_columns, dtype=float)
            torque_df = torque_df.assign(Trial=np.concatenate(torque_trial_cycle_names))
            torque_df.name = key + ' torque_raw_data'

            self.update_pandas(emg_df)
            self.update_pandas(torque_df)

    def cut_data_to_cycles(self, df, name, t_round_decim=2, add_time=None):
        df_copy = df.copy()

        data_to_cache = []
        trial_names = []

        for group, df in df_copy.groupby('Trial'):
            ids = group.split()
            df.pop('Trial')
            if group not in self.gait_cycles_dict.keys():
                warnings.warn('Gait cycles for trial ' + group + ' was not found in gait_cycles_dict.\n' +
                              'The trial was excluded... to include it make sure to put the cycle into the '
                              'gait_cycles_dict and rerun the function')
                continue
            cycles_dict = self.gait_cycles_dict[group]['gait_cycles']
            for cycle in cycles_dict:
                if add_time is not None:
                    if not type(add_time) == float:
                        warnings.warn("add_time should be a float holding time in seconds")
                        return
                    start_time = cycles_dict[cycle]['Start'] - add_time
                else:
                    start_time = cycles_dict[cycle]['Start']

                data_cycle = df.to_numpy()[(start_time <= df.to_numpy()[:, 0]) &
                                           (df.to_numpy()[:, 0] <= cycles_dict[cycle]['End'])]
                data_cycle[:, 0] = (np.array(data_cycle[:, 0]).astype(dtype=float) -
                                    cycles_dict[cycle]['Start']).round(decimals=t_round_decim)
                trial_names.append([ids[0] + ids[2] + cycle]*len(data_cycle))
                data_to_cache.append(data_cycle)

        columns_wo_trial = list(df_copy)
        columns_wo_trial.pop(df_copy.columns.get_loc('Trial'))
        df_cut = pd.DataFrame(data=np.concatenate(data_to_cache), columns=columns_wo_trial)
        df_cut = df_cut.assign(Trial=np.concatenate(trial_names))
        df_cut.name = name
        self.update_pandas(df_cut)

    def add_pandas(self, df, name):
        """
        Adds a pandas DataFrame to the self.list_of_pandas as well as saves it to cache

        :param pandas.DataFrame df: a pandas DataFrame typically holding experiment data
        :param str name: the name to use for the dataframe
        """
        if name in self.list_of_pandas.keys():
            warnings.warn('A dataframe with the name ' + name + ' already exists')
            return

        df.name = name
        df.to_pickle(cache_path + 'dataframes/' + name + '.pkl')
        self.list_of_pandas[name] = df

    def update_pandas(self, df, rename=None):
        """
        Updates a dataframe and stores it in cache, for example if it has been changed or if the name of it should be
        changed. If a dataframe with the same name does not already exist then the dataframe df is added to the list and
        cache. See :py:func:`DataManager.add_pandas`

        :param pandas.DataFrame df:
        :param str rename:
        """
        if df.name not in self.list_of_pandas.keys():
            self.add_pandas(df, df.name)
            logging.info('The dataframe ' + df.name + ' did not exists, used add_pandas(df, name) instead')
            return

        if rename is not None:
            self.remove_pandas(df)
            self.add_pandas(df, rename)
        else:
            df.to_pickle(cache_path + 'dataframes/' + df.name + '.pkl')
            self.list_of_pandas[df.name] = df

    def remove_pandas(self, df):
        """
        Deletes the pandas DataFrame from the self.list_of_pandas and cache, if it exists.

        :param pandas.DataFrame df: a pandas DataFrame (or the name of it) typically holding experiment data. If df is
        a string then every DataFrame that has a name where df is a substring, will be deleted.
        """
        if isinstance(df, pd.DataFrame):
            os.remove(cache_path + 'dataframes/' + df.name + '.pkl')
            if df.name in self.list_of_pandas.keys():
                del self.list_of_pandas[df.name]
        elif isinstance(df, str):
            for f in Path(cache_path + 'dataframes/').glob('*' + df + '*.pkl'):
                os.remove(str(f))
            for key in list(self.list_of_pandas.keys()):
                if df in key:
                    del self.list_of_pandas[key]

    def load_pandas(self):
        """
        Loads and stores all pandas DataFrames from the <cache_path>/dataframes/ directory to self.list_of_pandas
        """
        files = Path(cache_path + 'dataframes/').glob('*.pkl')

        for file in files:
            df = pd.read_pickle(str(file))
            df.name = file.stem
            self.list_of_pandas[file.stem] = df


def save_raw_data_to_txt(data, file_path, data_fmt='%f', headers=None):
    if headers:
        np.savetxt(file_path, data, fmt=data_fmt,
                   header=' '.join(emg_id for emg_id in headers), comments='')
    else:
        np.savetxt(file_path, data, fmt=data_fmt)


def clear_raw_data_cache(session_id=None):
    """
    Deletes all .txt files in the <cache_path>/raw_data that contain the string passed, typically session id.
    With a lot of trials this cache folder will take up a lot of memory and the files are typically never used after
    the data has been stored to a dataframe in the <cache_path>/dataframes directory.

    :param str session_id: Can be any string in the file names that should be deleted, typically session_id to delete
    all files from a specific session. If None, then all files will be deleted, (default: None).
    """
    if session_id is None:
        inputs_to_delete = Path(cache_path + 'raw_data/input/').glob('*.txt')
        labels_to_delete = Path(cache_path + 'raw_data/labels/').glob('*.txt')
    else:
        inputs_to_delete = Path(cache_path + 'raw_data/input/').glob('*' + session_id + '*.txt')
        labels_to_delete = Path(cache_path + 'raw_data/labels/').glob('*' + session_id + '*.txt')

    for file in inputs_to_delete:
        os.remove(str(file))

    for file in labels_to_delete:
        os.remove(str(file))


# Creates the new subject dictionary including all sessions and experiments
def create_subject(path, subject_id):
    sessions = []
    session_folders = Path(path + '\\' + subject_id).glob('*')
    for folder in session_folders:
        try:
            sessions.append(create_session(str(folder), subject_id))
        except AssertionError as error:
            warnings.warn(str(error) + "\n\tThe session was ignored!")
            continue

    return {"SubjectID": subject_id, "Sessions": sessions}


def update_subject(path, subject):
    session_ids = get_list_of_session_ids(subject)
    session_folders = Path(path).glob('*')
    for folder in session_folders:
        session_id = folder.stem

        if session_id not in session_ids:
            try:
                subject["Sessions"].append(create_session(str(folder), subject["SubjectID"]))
            except AssertionError as error:
                warnings.warn(str(error) + '\n\tSession ignored and not added to data_structure.json!')
        else:
            i = session_ids.index(session_id)
            subject["Sessions"][i]["Trials"] = update_trials(
                str(folder) + '\\' + subject["SubjectID"], subject["Sessions"][i], subject["SubjectID"])

    return subject


def create_session(path, subject_id):
    assert (os.path.exists(path + '\\session_config.json')),  "No session_config.json file found in: " + str(path)
    with open(path + '\\session_config.json', 'r') as tc:
        session = json.load(tc)

    trials = []
    trial_files = Path(path + '\\' + subject_id).glob('*.csv')
    for file in trial_files:
        trial_name = file.stem.replace(subject_id + ' ', '')
        if 'walk' in trial_name.lower():
            ex_type = 'inverse_dynamics'
        else:
            ex_type = 'emg_only'
        trials.append({"TrialID": trial_name, "Type": ex_type, "File": str(file)})

    session['Trials'] = trials

    return session


def update_trials(path, session, subject_id):
    session["Trials"] = []  # Delete previous list in case filenames have been changed or files deleted
    trial_files = Path(path).glob('*.csv')
    for file in trial_files:
        trial_id = file.stem.replace(subject_id + ' ', '')
        if 'walk' in trial_id.lower():
            ex_type = 'inverse_dynamics'
        else:
            ex_type = 'emg_only'
        session["Trials"].append({"TrialID": trial_id, "Type": ex_type, "File": str(file)})

    return session["Trials"]


def get_list_of_subject_ids(data_structure):
    subject_ids = []
    for subject in data_structure:
        subject_ids.append(subject["SubjectID"])

    return subject_ids


def get_list_of_session_ids(subject):
    session_ids = []
    for session in subject["Sessions"]:
        session_ids.append(session["SessionID"])

    return session_ids


def get_list_of_trial_ids(session):
    trial_ids = []
    for trial in session["Trials"]:
        trial_ids.append(trial["TrialID"])

    return trial_ids


def get_subject_structure(data_structure, subject_id):
    for subject in data_structure:
        if subject["SubjectID"] == subject_id:
            return subject


def get_torque_from_csv(file, torque_id, frame_freq):
    if not os.path.exists(file):
        raise Exception('The file ' + file + ' could not be found!')

    try:
        f = open(file, 'r')
    except IOError:
        print('Could not read file: ', file)
        sys.exit()

    torque_data = []
    with f:
        fl = f.readline()
        while 'Model' not in fl:
            fl = f.readline()

        f.__next__()

        reader = csv.reader(f, delimiter=',')

        model_output_list = next(reader)
        assert (torque_id in model_output_list), "No moment model output found in file: " + file
        first_col = model_output_list.index(torque_id)

        next(reader)
        next(reader)

        for line in reader:
            if len(line) < 10:
                f.close()
                break
            else:
                torque_data.append(line[:2] + line[first_col:first_col + 3])

    torque_data = np.array(torque_data, dtype=np.float)
    t = (torque_data[:, 0] / frame_freq)

    headers = ['Time', 'MomentX', 'MomentY', 'MomentZ']

    return np.concatenate((t.reshape(t.shape[0], 1), torque_data[:, 2:]), axis=1), headers


def get_emg_from_csv(file, emg_device, num_emg, frame_freq, analog_freq):
    """
    Iterates through the .csv file generated by Vicon Nexus and extracts the emg signals from given emg channels

    :param str file: full path to the .csv file containing the trial data
    :param str emg_device: the name of the emg device used in Vicon Nexus
    :param int num_emg: the number of emg channels used
    :param int frame_freq: the frequency of the motion lab cameras and thus the model outputs, i.e. moment data frequency
    :param int analog_freq: the sampling frequency of the emg channels
    :return: numpy array with time in the first column and the emg signals in the rest of the columns. The shape is thus
    (n, m) where n=the number of emg samples and m=1+num_emg
    """
    if not os.path.exists(file):
        raise Exception('The file ' + file + ' could not be found!')

    try:
        f = open(file, 'r')
    except IOError:
        print('Could not read file: ', file)
        sys.exit()

    emg_data = []
    with f:
        fl = f.readline()
        while 'Devices' not in fl:
            fl = f.readline()

        f.__next__()

        reader = csv.reader(f, delimiter=',')

        devices = next(reader)
        emg_first_col = devices.index(emg_device)
        headers = next(reader)
        headers = headers[emg_first_col:emg_first_col+num_emg]
        next(reader)
        next(reader)

        for line in reader:
            if len(line) < num_emg + 1 or '' in line[emg_first_col:emg_first_col + num_emg]:
                f.close()
                break
            else:
                emg_data.append(line[:2] + line[emg_first_col:emg_first_col + num_emg])

    emg_data = np.array(emg_data, dtype=np.float)
    t = (emg_data[:, 0] + (emg_data[:, 1] / (analog_freq / frame_freq))) / frame_freq

    return np.concatenate((t.reshape(t.shape[0], 1), emg_data[:, 2:]), axis=1)


def get_gait_cycles(file, leg='Right'):
    """Reads the time-frame of the gait cycles of either leg (default right) from the .csv file exported from Nexus

    :param file: the full path to the csv file
    :param leg: sets which leg to look at, can be 'Right' or 'Left', (default 'Right')
    :return: the strings t1, t2 which are respectively the start and stop time of gait cycle activity of the right leg
    """
    assert (os.path.exists(file)), 'The file ' + str(file) + ' could not be found!'

    try:
        f = open(file, 'r')
    except IOError:
        print('Could not read file: ', file)
        sys.exit()

    with f:
        reader = csv.reader(f, delimiter=',')
        row = next(reader)

        # Find events
        while 'Events' not in row:
            row = next(reader)

        next(reader)
        event_col_headers = next(reader)
        event_context_col = event_col_headers.index('Context')
        event_name_col = event_col_headers.index('Name')
        event_time_col = event_col_headers.index('Time (s)')

        right_fp_time = []
        time_value = 0
        row = next(reader)
        while 'General' == row[event_context_col]:
            if leg + '-FP' == row[event_name_col]:
                time_value = row[event_time_col]
                right_fp_time.append(time_value)
            row = next(reader)
            while row[event_time_col] == time_value:
                row = next(reader)

        assert (len(right_fp_time) > 0), 'No ' + leg.lower() + ' foot FP event detected: ' + file

        while leg != row[event_context_col]:
            row = next(reader)

        gait_cycles_name = []
        gait_cycles_time = []
        while leg in row:
            gait_cycles_name.append(row[event_name_col])
            gait_cycles_time.append(row[event_time_col])
            row = next(reader)

        # Define cycle(s) into dictionary
        right_fp_time = np.array(right_fp_time, dtype=np.float)
        gait_cycles_time = np.array(gait_cycles_time, dtype=np.float)
        gait_cycle_dict = {}
        faulty_cycle_num = 0
        for i in range(len(right_fp_time)):
            items_before_fp_max, = np.where(gait_cycles_time < right_fp_time[i])
            items_after_fp_max, = np.where(gait_cycles_time > right_fp_time[i])
            if items_before_fp_max.size > 0 and items_after_fp_max.size > 1:
                faulty_cycle = not (
                        gait_cycles_name[items_before_fp_max[items_before_fp_max.size-1]] == 'Foot Strike' and
                        gait_cycles_name[items_after_fp_max[0]] == 'Foot Off' and
                        gait_cycles_name[items_after_fp_max[1]] == 'Foot Strike'
                )
                if not faulty_cycle and np.abs(gait_cycles_time[items_before_fp_max[items_before_fp_max.size-1]] -
                                               gait_cycles_time[items_after_fp_max[1]]) > 2.50:
                    warnings.warn('Long gait cycle (longer than 2.5sec) in file: ' + file)
                    faulty_cycle = True
                if not faulty_cycle and items_before_fp_max.size > 1:
                    if np.abs(gait_cycles_time[items_before_fp_max[items_before_fp_max.size-1]] -
                              gait_cycles_time[items_before_fp_max[items_before_fp_max.size-2]]) < 0.30:
                        warnings.warn('Double heel strike in file: ' + file)
                        faulty_cycle = True
                if not faulty_cycle and items_after_fp_max.size > 2:
                    if np.abs(gait_cycles_time[items_after_fp_max[2]] - gait_cycles_time[items_after_fp_max[1]]) < 0.30:
                        warnings.warn('Double heel strike in file: ' + file)
                        faulty_cycle = True
            else:
                faulty_cycle = True

            if faulty_cycle:
                faulty_cycle_num = faulty_cycle_num + 1
                continue

            gait_cycle_dict['Cycle' + str(i + 1 - faulty_cycle_num)] = {
                'Start': np.around(gait_cycles_time[items_before_fp_max[len(items_before_fp_max)-1]], decimals=2),
                'End': np.around(gait_cycles_time[items_after_fp_max[1]], decimals=2)
            }

        assert (len(right_fp_time)-faulty_cycle_num > 0), 'All ' + leg.lower() + ' foot cycles defective in: ' + file

        while 'Devices' not in row:
            row = next(reader)
        fs = float(next(reader)[0])

        while 'Frame' not in row:
            row = next(reader)

        next(reader)
        row = next(reader)
        first_frame = ((float(row[0]) * 10) + float(row[1])) / fs

        while len(row) > 1:
            frame = row[0]
            sub_frame = row[1]
            row = next(reader)

        last_frame = ((float(frame) * 10) + float(sub_frame)) / fs

        t1 = 1000.0
        t2 = 0.0
        for cycle in list(gait_cycle_dict.keys()):
            if gait_cycle_dict[cycle]['Start'] < first_frame or gait_cycle_dict[cycle]['End'] > last_frame:
                gait_cycle_dict.pop(cycle)
            else:
                if gait_cycle_dict[cycle]['Start'] < t1:
                    t1 = gait_cycle_dict[cycle]['Start']
                if gait_cycle_dict[cycle]['End'] > t2:
                    t2 = gait_cycle_dict[cycle]['End']

        assert gait_cycle_dict, 'No "good" ' + leg.lower() + ' foot cycles found in file: ' + file

    return t1, t2, gait_cycle_dict
